{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_lhzUjEU052V",
    "nbgrader": {
     "checksum": "26e6840d18bee9b2d3a0d20b9feb48ad",
     "grade": false,
     "grade_id": "cell-461fb8635becf7f2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Neural Network for Digit Classification\n",
    "\n",
    "Author: Emily Cheng-hsin Wuu\n",
    "\n",
    "**Maximum possible score = 100 points**\n",
    "\n",
    "### Assignment Overview\n",
    "Deep learning has quickly become one of the more popular applied machine learning techniques in\n",
    "computer vision. Convolutional neural networks have been applied to many different computer vision problems such as image classification, recognition, and segmentation with great success. \n",
    "\n",
    "In this assignment, you will first implement a fully-connected feed-forward neural network for hand-written digit classification, which involves going into the details of network parameter initialization, activation functions, loss functions and gradient computation. You will go through the forward pass and the backward propagation demistify neural networks. Next, you  will move your work to PyTorch, where you can learn to use a modern deep learning framework to classify digits. \n",
    "\n",
    "### Learning Objectives \n",
    "* Describe the structure and function of neural networks and use a deep neural network to classify hand-written digits.\n",
    "\n",
    "### Source\n",
    "\n",
    "This assignment was derived, in part, from assignments used in conjunction with computer vision courses at Carnegie Mellon University: \n",
    "\n",
    "* 16-385 Computer Vision: Digit Recognition with Convolutional Neural Networks\n",
    "    * Authors: *Abhinav Garlapati and Prakruti Gogia*\n",
    "* 16-720A Computer Vision: Optical Character Recognition using Neural Networks\n",
    "    * Authors: *Chen-Hsuan Lin, Brian Okorn, Yifan Xing, Sree Harsha Kalli, Siddarth Malreddy, Prakhar Pradeep Naval, Khushi Gupta, Shangxuan Wu, Bala Siva Jujjavarapu, Jingyan Wang, Yashasvi Agrawal*\n",
    "* 16720-A S18 OCR using Neural Networks\n",
    "    * Authors: *Leonid Keselman, Mohit Sharma, Arjun Sharma, Rawal Khirodkar, Aashi Manglik, Tanya Marwah*\n",
    "* 16720-A F18 Neural Networks for Recognition\n",
    "    * Authors: *Leonid Keselman, Tanya Marwah, Shashank Tripathi, Vibha Nasery, Jiayuan Li*\n",
    "* 16720-B F18 Neural Networks for Recognition\n",
    "    * Authors: *Gaurav Mittal, Akshita Mittel, Sowmya Munukutla, Nathaniel Chodosh, Ming-Fang Chang, Chengqian Che*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Ip-vNwRM052V",
    "nbgrader": {
     "checksum": "6539537dfaf911564abb74fdb0e60e38",
     "grade": false,
     "grade_id": "cell-91fea75f52783136",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Table of Contents \n",
    "\n",
    "#### [Part I: Implement a Fully-connected Neural Network [100 pts]](#part-1)\n",
    "- [Item 1 [0 pts]: Network Initialization (Xavier)](#q1)\n",
    "- [Item 2 [10 pts]: Sigmoid Activation Function](#q2)\n",
    "- [Item 3 [15 pts]: Softmax Function](#q3)\n",
    "- [Item 4 [15 pts]: Loss Function (Cross-Entropy)](#q4)\n",
    "- [Item 5 [20 pts]: Forward Pass](#q5)\n",
    "- [Item 6 [20 pts]: Gradient Computation](#q6)\n",
    "- [Item 7 [0 pts]: Backward Propagation](#q7)\n",
    "- [Item 8 [20 pts]: Train Models with NIST36](#q8)\n",
    "\n",
    "#### [Part 2: PyTorch for Digit Classification [0 pts]](#part-2)\n",
    "- [Item 9 [0 pts]: PyTorch Installation](#q9)\n",
    "- [Item 10 [0 pts]: Train Models with NIST36 (PyTorch)](#q10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gC_SMiinZWVa"
   },
   "outputs": [],
   "source": [
    "#import Python libraries for this assignment\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wZZKsa8lTAON",
    "nbgrader": {
     "checksum": "44f7775e85a73e32ec8cb2c16cf2b58d",
     "grade": false,
     "grade_id": "cell-171479d7532dd376",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"part-1\"></a>\n",
    "# Part I: Implement a Fully-connected Neural Network [ 45 pts]\n",
    "We will give a brief overview of the math for a feed forward network with a single hidden layer. There are two parts:  (1) forward propagation where the network learns from the input data; (2) backward propagation where the network updates its parameters based on the loss function. This will help you to understand the implementation of different functions required for building a simple two-layer neural network at the end of this section. \n",
    "\n",
    "## Forward propagation\n",
    "To do classifcation with a fully-connected network $f$, you need to apply a series of linear and non-linear\n",
    "functions to an input vector $x$ of size $N \\times 1$ to generate an output vector $f(x)$ of size\n",
    "$C \\times 1 $, where each element of the output vector represents the probability of input vector $x$ belonging to certain class in the dataset. The input layer has $N$ input units as the dimension of the data samples is $N$. In between the input and output layers, we need to compute the values of each units from all of the hidden layers. For any unit $x$ from the <i><b>pre-activated</b></i> hidden layers, we can write it as \n",
    "\n",
    " <center>$a^{(1)}(x) = W^{(1)}x + b^{(1)}$</center>\n",
    " \n",
    "Next, it will go through an activation function $g$, and the <i><b>post-activated</b></i>values of the hidden layer $h^{(1)}(x)$ will be:\n",
    "\n",
    "<center>$h^{(1)}(x)= g(a^{(1)}(x)) = g(W^{(1)}x + b^{(1)})$ </center>\n",
    "\n",
    "Following this pattern, we can write any hidden layer ($1 < t \\leq T $) pre- and post- activations by:\n",
    "\n",
    " <center>$a^{(t)}(x) = W^{(t)}h^{(t-1)} + b^{(t)}$ </center>\n",
    " \n",
    " <center>$h^{(t)}(x) = g(a^{(t)}(x))$ </center>\n",
    " \n",
    "Finally, the  <i><b>post-activation</b></i> values of the output layer are computed with:\n",
    "\n",
    "<center>$f^{(x)} = o(a^{(T)}(x)) = o(W^{(T)}h^{(T - 1)}(x) + b^{(T)})$</center>\n",
    "\n",
    "where $o$ is the output activation function. For most of the case activation function $g$ and $o$ will be different. For\n",
    "this assignment, you will be using the <i><b>sigmoid activation function</b></i>  for the hidden layer (activation function $g$):\n",
    "<center>$g(y) = \\frac{1}{1 + exp(y)}$</center>\n",
    "\n",
    "where when $g$ is applied to a vector, it is applied element wise across the vector.\n",
    "\n",
    "\n",
    "Since you are working on classification tasks, you will use <i><b>softmax function</b></i> as the output activation function ($o$) to turn the real value, possibly negative\n",
    "values of $a^{(T)}(x)$ into a set of probabilities (vector of positive numbers that sum to 1) that represents the possibility of each element belonging to certain class in the dataset. Letting\n",
    "$x_{i}$ denote the $i$th element of the vector $x$, the softmax function is defined as:\n",
    "<center>$o_{i}(y) = \\frac{exp(y_{i})}{\\sum_{j}{exp(y_{j})}} $</center>\n",
    "\n",
    "\n",
    "After your network outputs the predicting probabilities of each class, you arrive at your final destination of the forward propagation: computing the loss function. Here, the loss function is used to compute the difference between the ground truth (true value), and the prediction from the network. A properly-functioning network should have the prediction as close to the ground truth as possible, meaning that the difference of these two terms should be somewhere close to zero. For this programming assignment, you will use <i><b>cross-entropy loss</b></i>, which is generally used for classification tasks: \n",
    "<center>$L_{f}(D) = -\\sum_{(x, y)\\in D}{y \\cdot log(f{x})}$</center>\n",
    "\n",
    "In the later sections, you will see more details about the intuitive explanations for these different functions.\n",
    "\n",
    "## Backward propagation\n",
    "After each forward propagation, you need a method to update the network parameters based on the loss function. Here, gradient descent is an iterative optimization algorithm, used to find the local optima. To\n",
    "find the local optima, we randomly select a point on the function and move in the direction of\n",
    "negative gradient until some stopping criteria are met (eg. loss doesn't change for a long time). To compute the gradient, you need to compute the derivative of the loss function over the network parameters (weights and biases). Mathmatically, the update equation for a general weight $W_{ij}^{t}$ abd bias $b_{i}^{t}$ is:\n",
    "<center>$W_{ij}^{t)} = W_{ij}^{t-1} - \t\\alpha * \\frac{\\sigma L_{f}}{\\sigma W_{ij}^{t-1}}$</center>\n",
    "<center>$b_{i}^{t} = b_{i}^{t-1} - \t\\alpha * \\frac{\\sigma L_{f}}{\\sigma b_{i}^{t-1}}$</center>\n",
    "\n",
    "where $\\alpha$ is the learning rate. \n",
    "\n",
    "## Let's Get Started! \n",
    "After knowing the math behind building the neural network, you will now start to put everything together, which includes initialization, the activation function, the loss function for forward pass, and gradient descent for backward propagation. At the end of Part 1, you will build a simple two-layer neural network from scratch and train your network to do digit classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "154c3f1d6c274e388df3afa338f2e9fa",
     "grade": false,
     "grade_id": "cell-e140164d871d99dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q1\"></a>\n",
    "## Network Initialization [ 0 pts]\n",
    "\n",
    "Initialization of network weights is one of the more important techniques for training network successfully. You might think about why you couldn't just directly initalize the weights with all zeros. The reason is because initializing all the parameters (weights and biases) in the network to zero will result in the output of every neuron\n",
    "become zero during the forward propagation, meaning that when you do the back\n",
    "propagation, you will get same gradient for all the parameters as you feed them with the\n",
    "same input. Hence, it is impossible for us to update our parameters and get closer and closer to the optimal solution.\n",
    "\n",
    "To properly handle initialization, you will use <a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\">Xavier initialization</a> - formula (16), where $Var[weight] = \\frac{2}{(n_{in} + n_{out})}$, where $n_{in}$ is the dimensionality of the input layer and $n_{out}$ is the dimensionality of the output layer. You will use a uniform distribution $weight \\sim U[-\\frac{\\sqrt 6}{\\sqrt (n_{in} + n_{out})}, \\frac{\\sqrt 6}{\\sqrt (n_{in} + n_{out})}]$to sample random numbers. The function is provided as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(n_in, n_out, params, name=''):\n",
    "    \"\"\"\n",
    "    Xavier weight initialization\n",
    "    \n",
    "    :param n_in: input vector [dimension of input layer]\n",
    "    :param n_in: output vector [dimension of output layer]\n",
    "    :param params: a dictionary containing parameters\n",
    "    :param name: name of the layer\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    b = np.zeros(n_out)\n",
    "    n = n_in + n_out\n",
    "    l, u = -np.sqrt(6/n), np.sqrt(6/n)\n",
    "    W = np.random.uniform(l, u, (n_in, n_out))\n",
    "    \n",
    "    ## store parameters of the network\n",
    "    params['W' + name] = W\n",
    "    params['b' + name] = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb56796d76a57db87e874aff789dbe47",
     "grade": false,
     "grade_id": "cell-9d9cebf78bd05106",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q2\"></a>\n",
    "## Sigmoid Activation Function [ 10 pts]\n",
    "Now, you will move on implementing the forward propagation for your neural network, where you need to implement activation funtions. Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated or not, based on whether each neuron’s output is relevant for the model’s prediction. \n",
    "\n",
    "You want to implement the sigmoid activation function, along with forward propagation for a single layer with an activation function, namely $y = \\sigma (XW + b)$, returning the output and intermediate results for an $N \\times D$ dimension input $X$, with examples ($N$ dimension) along the rows, data dimensions ($D$ dimension) along the columns. For example, if our $X$ consists of 10 points under 2d coordinate representing by $(x, y)$, then $X$ will have dimension as 10 x 2 ($N=10, D=2$) \n",
    "If you visualize the sigmoid activation function, it will look like the figure below. Notice the range of the output is clamped between zero and one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#example of the sigmoid activation function appearance\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = 1/(1 + np.exp(-x))\n",
    "  \n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"input\")\n",
    "plt.ylabel(\"output = sigmoid(input)\")\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d4218d77cc3f6f739389e5bbadf3ceba",
     "grade": false,
     "grade_id": "cell-ace632bbdb57b823",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Follow the math formula of sigmoid activation: \n",
    "<center>$\\sigma (x) = \\frac{1}{1 + exp(x)}$</center>\n",
    "\n",
    "to implement the following function with NumPy. You can call $np.exp(x)$ to express exponential function of $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "03c12910ed7f458c2e9f635a0da56732",
     "grade": false,
     "grade_id": "cell-2fe40d375c495695",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "\n",
    "\n",
    "    :param x: input 2d vectors (N x D), where N refers to number of data, \n",
    "        and D refers to number of dimension of each input data.\n",
    "    :return: sigmoid activation function.\n",
    "    \"\"\"\n",
    "    sigmoid = 1/(1+ np.exp(-x)) #to be implemented\n",
    "    return sigmoid\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d4d2959303e7ff565f0e569508dc44c",
     "grade": true,
     "grade_id": "cell-a9352f55d5817ca4",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a2e7ac4d1dccea0e82a502012bb419c4",
     "grade": false,
     "grade_id": "cell-6676dd6ff5f20b8d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q3\"></a>\n",
    "## Softmax Function [ 15 pts]\n",
    "Since we are using neural networks for classification, a common output activation function\n",
    "to use for the last layer is the softmax function. This will allow us to turn the real value, possibly negative\n",
    "values into a set of probabilities (vector of positive numbers that sum to 1). Implement the softmax function, which is defined as below for each index $i$ in a vector $x$:\n",
    "<center>$\\text{softmax}(x_{i}) = \\frac{exp(x_{i})}{\\sum_{j}{exp(x_{j})}} $</center>\n",
    "\n",
    "\n",
    "Here, we usually add an offset $c = -max (x_{i})$ to the input $x_i$ to make sure the output of the numerator is within the range $(0, 1]$. If we don't apply any offset $c$, meaning that $c=0$ in this formula, then the output range of the numerator will be $(0, infinite]$, which might cause an overflow calculation. This trick utilizes the fact that softmax function is invariant to translation, that is:\n",
    "<center>$\\text{softmax}(x) = \\text{softmax}(x + c), \\forall c \\in \\mathbb{R} $</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a49a417a9b77bced789b4c43ae2e8ee7",
     "grade": false,
     "grade_id": "cell-9db80528c9af83bc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "\n",
    "    :param x: input 2d vectors (N x C), where N refers to number of data, \n",
    "        and C refers to number of possible classes in the dataset.\n",
    "    :return: softmax activation function\n",
    "    \"\"\"\n",
    "    # remember to apply trick\n",
    "    offset_c = np.max(x, axis=1, keepdims=True)  # Get max value for each row and keep the dimension #to be implemented\n",
    "    exp_values = np.exp(x - offset_c)\n",
    "    exp_sums = np.sum(exp_values, axis=1, keepdims=True)\n",
    "    # softmax should be done for each row (data by data)\n",
    "    softmax = exp_values / exp_sums #to be implemented\n",
    "    \n",
    "    return softmax\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9601cd180d61d43f204cd14c72798e2d",
     "grade": true,
     "grade_id": "cell-9ac07e0eba6cf95c",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c17a411a3b8b597af08a57b1c37132e1",
     "grade": false,
     "grade_id": "cell-ae6460ada354aefa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q4\"></a>\n",
    "## Loss Function (Cross-Entropy)[ 15 pts]\n",
    "In this last part of the forward pass, you want to evaluate the prediction of the network with its ground truth labels. The loss function generally used for classification is the\n",
    "cross-entropy loss, which is defined as the following:\n",
    "<center>$L_{f}(D) = -\\sum_{(x, y)\\in D}{y \\cdot log(f{x})}$</center>\n",
    "\n",
    "\n",
    "where $D$ is the full training dataset of data samples $x (N \\times 1 $ vectors, $N =$ dimensionality of\n",
    "data) and labels $y (C \\times 1 $ one-hot vectors, $C = $number of classes).\n",
    "\n",
    "Also, you'll need to know the accuracy of the current performance, which is defined as the number of correct predictions over all predictions. That is, it's the proportion of correct predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a6e21cd2785dcbe10c3d040b4933b059",
     "grade": false,
     "grade_id": "cell-3303394ca034c70b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def compute_loss_and_acc(y, probs):\n",
    "    \"\"\"\n",
    "    Cross entropy loss and accuracy \n",
    "\n",
    "    :param y: ground truth labels. 2d vectors (N x C), where N refers to number of data, \n",
    "        and C refers to number of possible classes in the dataset.\n",
    "    :param probs: prediction from netowkr output. 2d vectors (N x C), where N refers \n",
    "        to number of data, and C refers to number of possible classes in the dataset.\n",
    "    :return: loss and accuracy of the prediction.\n",
    "    \"\"\"\n",
    "    loss = -np.sum(y * np.log(probs))    \n",
    "    if len(probs.shape) < 2:\n",
    "        probs = np.array([probs])\n",
    "    if len(y.shape) < 2:\n",
    "        y = np.array([y])\n",
    "    acc = np.mean(np.argmax(probs ,axis=1) == np.argmax(y, axis=1))\n",
    "    return loss, acc\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "37062203d3c562cd49c9b8bca02a20ae",
     "grade": true,
     "grade_id": "cell-191558d10a69ad54",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "413ea123c7044339a3840f46f30f2fcf",
     "grade": false,
     "grade_id": "cell-3f9da92703220b7e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q5\"></a>\n",
    "## Forward Propagation[ 20 pts]\n",
    "You now have all the components needed for computing the forward pass, which should be as follows:\n",
    "\n",
    "1. Input ($X$) $\\rightarrow$ \n",
    "2. Linear Layer* ($Y = XW + B$) $\\rightarrow$ \n",
    "3. Activation Layer* ($Y = \\sigma (XW + b)$) $\\rightarrow$ \n",
    "4. Loop through Linear Layer (step 2) then Activation layer (step 3) $\\rightarrow$ \n",
    "5. Output Layer (softmax) $\\rightarrow$ \n",
    "6. Compute Loss and Accuracy \n",
    "\n",
    "*Here, you'll want to modularize the \"Linear Layer ($Y = XW + B$) $\\rightarrow$ Activation Layer ($y = \\sigma (XW + b)$)\" part so you can reuse it. \n",
    "\n",
    "Please note the use of the `name` argument which is used for clarification to label the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e1721572d0060e130e17c0fdaa51a0e3",
     "grade": false,
     "grade_id": "cell-ff796a0da38efdb1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def forward(X, params, name='', activation=sigmoid):\n",
    "    \"\"\"\n",
    "    Do a forward pass\n",
    "\n",
    "    :param X: 2d input vector(N x D), where N refers to number of data, \n",
    "        and D refers to number of dimension of each input data.\n",
    "    :param params: a dictionary containing parameters\n",
    "    :param name: name of the layer\n",
    "    :param activation: the activation function (default is sigmoid)\n",
    "    :return: 2d vector after pre-activation and post-activation.\n",
    "    \"\"\"\n",
    "    \n",
    "#     pre_act, post_act = None, None\n",
    "    \n",
    "    # get the layer parameters\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    \n",
    "    pre_act = np.matmul(X, W) + b\n",
    "    post_act = activation(pre_act)\n",
    "    ###############################\n",
    "    ##### Your Implementation #####\n",
    "    ###############################\n",
    "    \n",
    "#     pre_act = None # Linear Layer (Y = WX + B), to be implemented\n",
    "    \n",
    "    # Try to utilize the function implemented before\n",
    "#     post_act = None # Activation Layer (sigmoid), to be implemented\n",
    "\n",
    "    # store the pre-activation and post-activation values\n",
    "    # these will be important in backprop\n",
    "    params['cache_' + name] = (X, pre_act, post_act)\n",
    "    \n",
    "    return post_act\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4c4f342f5182056a394b0129177d0c29",
     "grade": true,
     "grade_id": "cell-097bffc41ec1425f",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "941ef02c7a1d40813935098d00f48204",
     "grade": false,
     "grade_id": "cell-728268f938c1eb58",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q6\"></a>\n",
    "## Gradient Computation[ 20 pts]\n",
    "\n",
    "After finishing the forward pass, now we can look closer into the backward pass, which will help optimize the netowrk by updating its parameters (weights and biases). Given an neural network and a loss function, backward proporgation calculates the gradient of the loss function with respect to the neural network's weights. For each non-linear function (eg. $Y=XW + b$), the derivative will just be 1 no matter what is the input, as the linear function is a line with constant slope = 1. For non-linear function, we need to compute the derivative manually however. In this assignment, we will only need to look into the derivative of sigmoid function, which can be derived as seen on [this page](https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x).\n",
    "\n",
    "Try to compute it again yourself, and implement the function below according to the formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dabd1601cd21c2d914db931074fe0253",
     "grade": false,
     "grade_id": "cell-46b0e91c61e69ac2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### First derivative of a linear function [ 10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f0da4714465fc8c34736e0ece47ee052",
     "grade": false,
     "grade_id": "cell-22ea229cc8906167",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "def linear_deriv(post_act):\n",
    "    \"\"\"\n",
    "    First derivative of linear function\n",
    "\n",
    "    :param post_act: activation function which we want to find it first derivative\n",
    "    :return: first derivative of linear function\n",
    "    \"\"\"\n",
    "    gradient = np.ones_like(post_act) #to be implemented\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ddfc1afba96a9bd22b4d6ddcd2c69f96",
     "grade": true,
     "grade_id": "cell-d900ffff767e0d50",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "42212aee332534d885f08e54de229c66",
     "grade": false,
     "grade_id": "cell-02b17961b527d027",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### First derivative of a sigmoid function [ 10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a08a62e1ea32572efd0c78b128dfe705",
     "grade": false,
     "grade_id": "cell-b05ab5df50b335d7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def sigmoid_deriv(post_act):\n",
    "    \"\"\"\n",
    "    First derivative of sigmoid function\n",
    "\n",
    "    :param post_act: activation function which we want to find it first derivative\n",
    "    :return: first derivative of sigmoid function\n",
    "    \"\"\"\n",
    "    gradient = post_act*(1.0-post_act) #to be implemented\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d2a453b0de87fbbaf7e528fa568fb2d2",
     "grade": true,
     "grade_id": "cell-4b178d926df6667a",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d5219ae4a09711343d7abc71d90f42ad",
     "grade": false,
     "grade_id": "cell-cd6b922c739c11af",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q7\"></a>\n",
    "## Backward Propagation[ 0 pts]\n",
    "You now have all the components necessary for computing the backward pass. You can apply the chain rule to backpropagate the gradient from the output to the input. \n",
    "\n",
    "Combining the gradient computation from before along with the update equation for a general weight $W_{ij}^{t}$ and bias $b_{i}^{t}$ yields:\n",
    "<center>$W_{ij}^{t)} = W_{ij}^{t-1} - \t\\alpha * \\frac{\\sigma L_{f}}{\\sigma W_{ij}^{t-1}}$</center>\n",
    "<center>$b_{i}^{t} = b_{i}^{t-1} - \t\\alpha * \\frac{\\sigma L_{f}}{\\sigma b_{i}^{t-1}}$</center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards(delta, params, name='', activation_deriv=sigmoid_deriv):\n",
    "    \"\"\"\n",
    "    Do a backwards pass\n",
    "\n",
    "    :param delta: errors to backprop #derivative loss\n",
    "    :param params: a dictionary containing parameters\n",
    "    :param name: name of the layer\n",
    "    :param activation_deriv: the derivative of the activation_func\n",
    "    :return: 2d vector after back proporgation\n",
    "    \"\"\"\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    X, pre_act, post_act = params['cache_' + name]\n",
    "\n",
    "    # do the derivative through activation first\n",
    "    # then compute the derivative W,b, and X\n",
    "    # grad_X: num data*insize, grad_W: insize*outsize, grad_b: outsize\n",
    "    grad_X, grad_W, grad_b = np.zeros(X.shape), np.zeros(W.shape), np.zeros(b.shape)\n",
    "    delta_post = delta*activation_deriv(post_act)\n",
    "    for i in range(X.shape[0]):\n",
    "        grad_W += np.matmul(np.expand_dims(X[i,:], axis=1), np.expand_dims(delta_post[i,:], axis=0))\n",
    "        grad_X[i,:] = np.matmul(W,delta_post[i,:])\n",
    "        grad_b += delta_post[i,:]\n",
    "        \n",
    "    # store the gradients\n",
    "    params['grad_W' + name] = grad_W\n",
    "    params['grad_b' + name] = grad_b\n",
    "    return grad_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c6fc652d49306061388465f295836cf4",
     "grade": false,
     "grade_id": "cell-9b1ce6ce0b8b4adb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q8\"></a>\n",
    "## Train Models with NIST [ 20 pts]\n",
    "You'll now test your model on the [NIST dataset](https://www.nist.gov/srd/nist-special-database-19) for hand-written digit classification. You'll need a training loop and a testing loop to put the forward pass and the backprop together for your two-layer network. \n",
    "\n",
    "The dimensions of each input image are 32 x 32 pixels. The image values are organized as a single 1024-dimensional vector which is multiplied by W(1). Because of this, each row of W(1) is a weight. The rows of W(1) can be reshaped into a 32 x 32 image in order to suggest to which types of images each hidden unit is sensitive.\n",
    "\n",
    "You are provided with three data `.mat` files to use for this section. \n",
    "\n",
    "The data in `nist36 train.mat` represent samples for each of the 26 upper-case letters of the alphabet and the 10 digits. You can use this set to train your network.\n",
    "\n",
    "The validation set in `nist36 valid.mat` contains samples from each class and should be used in the training loop to validate how well the network is performing on data on which it is not in the training set. This will help to identify [overfitting](https://en.wikipedia.org/wiki/Overfitting). \n",
    "\n",
    "The data in `nist36 test.mat` should be used for the final evaluation on your best model to see how well it will generalize to new unseen data.\n",
    "\n",
    "The following sections will go step-by-step describing how to implement the training and testing loops. At the end of this question, you will be able to visualize what your network learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "285610b97b75eca24689b04e2d4f8371",
     "grade": false,
     "grade_id": "cell-89d11c20941aa132",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "First, load your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "test_data = scipy.io.loadmat('data/nist36_test.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "test_x, test_y = test_data['test_data'], test_data['test_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fcf365214126426cec17094e8bf3f338",
     "grade": false,
     "grade_id": "cell-3ab5c03e56dd5b77",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Next, initialize your hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper-parameters for training loop, you can play with this number to see how it affects the network performance\n",
    "## number of max training iterations, batch size, learning rate, hidden layer size\n",
    "max_iters = 50\n",
    "batch_size = 32\n",
    "learning_rate = 9e-3\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed1715ea0ef59ebd6185bd64b5e4fe21",
     "grade": false,
     "grade_id": "cell-562a7a2180973d71",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Then, split your data into batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split x and y into random batches, constructing a list of [(batch1_x,batch1_y)...]\n",
    "index = np.arange(train_x.shape[0])\n",
    "np.random.shuffle(index)\n",
    "\n",
    "batches = [(train_x[index[i:i+batch_size]], train_y[index[i:i+batch_size]]) for i in range(0, train_x.shape[0], batch_size)]\n",
    "batch_num = len(batches)\n",
    "\n",
    "## A dictionary containing parameters of network\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c891782dc7fe9f697ccd32aac290994",
     "grade": false,
     "grade_id": "cell-b58b9733cacb5635",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Now, initialize your network layers [ 5 pts]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "07708d1a2dcb97487e8c471ab877aba8",
     "grade": false,
     "grade_id": "cell-0bc63256435f7cdd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "## initialize layers here \n",
    "input_size = 1024                                 #to be implemented\n",
    "output_size = 36                                #to be implemented\n",
    "initialize_weights(input_size, hidden_size, params, 'layer1')  #to be implemented\n",
    "initialize_weights(hidden_size,output_size, params, 'output') #to be implemented\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "340e198a32bc2cd6bcea24d5d302508b",
     "grade": true,
     "grade_id": "cell-b05f9f3c0d968b5a",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "673952616dc79be3f4ad3b6143296eeb",
     "grade": false,
     "grade_id": "cell-eff8b2ac64285e9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### In this cell, run your training loop [ 10 pts]. \n",
    "This might take a little while to run. You should see both training and validation data for each of the `max_iters` you set up earlier. With `max_iters=50`, you'll see 100 rows of data from this cell. Note: it's recommended to maintain the same `Name` argument you used in the earlier \"initialize weights\" part or you may get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "48a3c155c2005fa6155fe3cedfce1abe",
     "grade": false,
     "grade_id": "cell-030e19866bc85425",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training>> itr: 00 \t loss: 3.03 \t acc : 0.18\n",
      "validation>> itr: 00 \t loss: 2.22 \t acc : 0.45\n",
      "training>> itr: 02 \t loss: 1.53 \t acc : 0.57\n",
      "validation>> itr: 02 \t loss: 1.37 \t acc : 0.62\n",
      "training>> itr: 04 \t loss: 1.18 \t acc : 0.67\n",
      "validation>> itr: 04 \t loss: 1.21 \t acc : 0.65\n",
      "training>> itr: 06 \t loss: 1.02 \t acc : 0.71\n",
      "validation>> itr: 06 \t loss: 1.11 \t acc : 0.68\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "train_acc_list, valid_acc_list, train_loss_list,valid_loss_list = [], [], [], []\n",
    "\n",
    "# # with default settings, you should get loss < 0.5 and accuracy > 80%\n",
    "for itr in range(max_iters):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    for xb,yb in batches:  \n",
    "        ## training loop\n",
    "        # forward\n",
    "        out = forward(xb,params,'layer1')   #to be implemented\n",
    "        probs = forward(out,params,'output',softmax) #to be implemented\n",
    "\n",
    "        # loss\n",
    "        # be sure to add loss and accuracy to epoch totals\n",
    "        loss, acc = compute_loss_and_acc(yb, probs) #to be implemented\n",
    "        \n",
    "        # backward\n",
    "        delta = probs\n",
    "        y_idx = np.argmax(yb, axis=1)\n",
    "        delta[np.arange(probs.shape[0]),y_idx] -= 1\n",
    "        delta = backwards(delta, params, 'output', linear_deriv) #to be implemented\n",
    "        delta = backwards(delta, params, 'layer1', sigmoid_deriv) #to be implemented\n",
    "       \n",
    "        # apply gradient\n",
    "        for k,v in sorted(list(params.items())):\n",
    "            if 'grad' in k:\n",
    "                name = k.split('_')[1]\n",
    "                params[name] -= learning_rate*v\n",
    "\n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "    \n",
    "    total_loss /= train_x.shape[0]\n",
    "    total_acc /= len(batches)\n",
    "    \n",
    "    ## validation loop\n",
    "    out = forward(valid_x,params,'layer1')   #to be implemented\n",
    "    probs = forward(out,params,'output',softmax) #to be implemented\n",
    "    \n",
    "    valid_loss,valid_acc = compute_loss_and_acc(valid_y,probs) #to be implemented\n",
    "    valid_loss /= valid_x.shape[0]\n",
    "\n",
    "    train_acc_list.append(total_acc)\n",
    "    valid_acc_list.append(valid_acc)\n",
    "    train_loss_list.append(total_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    if itr % 2 == 0 or itr == max_iters - 1:\n",
    "        print(\"training>> itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(itr,total_loss,total_acc))\n",
    "        print(\"validation>> itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(itr,valid_loss,valid_acc))\n",
    "    if itr == 20 or itr == 40:\n",
    "        learning_rate /= 10.\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d91adda164846cd8e20583381641eaa8",
     "grade": true,
     "grade_id": "cell-dfbe35a3124bfc54",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fa4b382ce73263b9f2ae4c106f11c457",
     "grade": false,
     "grade_id": "cell-e3324bf039134bd5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You should visualize that data you just produced to see the trend for the training and validation accuracy and loss. Think about how these curves should look and then check your results in the plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the accuracy and loss along the iterations for both training and validaition set\n",
    "x = np.arange(max_iters)\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.plot(x, train_acc_list, linewidth = 2, label = \"Training Accuracy\")\n",
    "plt.plot(x, valid_acc_list, linewidth = 2, label = \"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, train_loss_list, linewidth = 2, label = \"Training Loss\")\n",
    "plt.plot(x, valid_loss_list, linewidth = 2, label = \"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d932b63530ef4cdf1cea99a36668bb23",
     "grade": false,
     "grade_id": "cell-63eeacd4e7ccbc57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Now take a look at the accuracy [ 5 pts]. \n",
    "This should be a relatively high number if your model is working well! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e1926274af6fc6cdc1b501d64e2f293a",
     "grade": false,
     "grade_id": "cell-4ff4bbd5d26d733f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "## run on validation set and report accuracy! should be above 75%\n",
    "\n",
    "out = forward(valid_x, params, 'layer1')                               #to be implemented\n",
    "probs = forward(out, params, 'output', softmax)                             #to be implemented\n",
    "valid_loss, valid_acc = compute_loss_and_acc(valid_y, probs)      #to be implemented\n",
    "print('Validation accuracy: ',valid_acc)\n",
    "\n",
    "outt = forward(test_x, params, 'layer1')                              #to be implemented\n",
    "probst = forward(outt, params, 'output', softmax)                            #to be implemented\n",
    "test_loss, test_acc = None, None         #to be implemented\n",
    "print('Test accuracy: ',test_acc)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb3a9e8316786aa2f7a1f527031a9113",
     "grade": true,
     "grade_id": "cell-d28c8cdf48449ea1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f1ea6bee1366a5de70fe7225326d7f9",
     "grade": false,
     "grade_id": "cell-f635b3120caf2ba1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Visualize the network\n",
    "Now you'll see what your network learned when it passes through different layers. Since your network is a simple two-layer network with only one hidden layer and an output layer, you can visualize the weights of network (1) right after initialization (2) from the hidden layer (3) from the output layer. Run the code below and try to describe what you discover! Does this visualization look organized to you?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualization of weights immediately after initialization\n",
    "new_params = {}\n",
    "initialize_weights(input_size, hidden_size, new_params,'layer1')\n",
    "initialize_weights(hidden_size, output_size, new_params,'output')\n",
    "\n",
    "fig = plt.figure(1, (15., 15.))\n",
    "if hidden_size < 128:\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(8, 8),  # creates 2x2 grid of axes\n",
    "                     axes_pad=0.1,  # pad between axes in inch.\n",
    "                     )\n",
    "    img_w = new_params['Wlayer1'].reshape((32,32,hidden_size))\n",
    "    for i in range(hidden_size):\n",
    "        grid[i].imshow(img_w[:,:,i])  # The AxesGrid object work as a list of axes.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "642a55ab91d95810a24409d80efc0a48",
     "grade": false,
     "grade_id": "cell-0f44ac359e3d3722",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Do another visualization using the output of the first layer. Does this look different to you? How so? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualization of first layer \n",
    "fig = plt.figure(1, (15., 15.))\n",
    "if hidden_size < 128:\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                    nrows_ncols=(8, 8),  # creates 2x2 grid of axes\n",
    "                    axes_pad=0.1,  # pad between axes in inch.\n",
    "                    )\n",
    "    img_w = params['Wlayer1'].reshape((32,32,hidden_size))\n",
    "    for i in range(hidden_size):\n",
    "        grid[i].imshow(img_w[:,:,i])  # The AxesGrid object work as a list of axes.\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a717b6cabc5a48727ee0312779d39a52",
     "grade": false,
     "grade_id": "cell-34cb54bb73363540",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now visualize the results of the output layer and compare the resulting plots to the ground truth images. How do these images look to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visulization of output layer \n",
    "## compare the ground truth and the prediction\n",
    "## do these learned plots seem to resemble the ground truth in any way?\n",
    "fig = plt.figure(1, (12., 16.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(12, 6),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "indices = params['cache_output'][2].argmax(axis=0)\n",
    "images = valid_x[indices]\n",
    "images = images.reshape(36, 32, 32)\n",
    "\n",
    "vis = np.zeros((36, 1024))\n",
    "inps = np.eye(36)\n",
    "for i,inp in enumerate(inps):\n",
    "    vis[i] = inp @ params['Woutput'].T @ params['Wlayer1'].T \n",
    "vis = vis.reshape(36, 32, 32)\n",
    "\n",
    "displayed = np.zeros((72, 32, 32))\n",
    "displayed[::2] = images\n",
    "displayed[1::2] = vis\n",
    "for ax, im in zip(grid, displayed):\n",
    "    ax.imshow(im.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0a142230d9c1ae1f8102b53ba3bdf70f",
     "grade": false,
     "grade_id": "cell-b80bc9819218770f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Confusion Matrix\n",
    "A confusion matrix is a technique for summarizing the performance of a classification algorithm. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.The confusion matrix shows the ways in which your classification model is confused when it makes predictions, and can tell you from where the model frequently make mistakes.\n",
    "\n",
    "Run the following code and point out a few pairs of classes that are most commonly confused. These will appear as highlighted regions or spots in the triangles of the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((train_y.shape[1],train_y.shape[1]))\n",
    "# compute confusion matrix here\n",
    "real=np.argmax(valid_y,axis=1)\n",
    "prediction=np.argmax(probs ,axis=1)\n",
    "for r,p in zip(real,prediction):\n",
    "    confusion_matrix[r][p]+=1\n",
    "\n",
    "import string\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "plt.imshow(confusion_matrix,interpolation='nearest')\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(36),string.ascii_uppercase[:26] + ''.join([str(_) for _ in range(10)]))\n",
    "plt.yticks(np.arange(36),string.ascii_uppercase[:26] + ''.join([str(_) for _ in range(10)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "56ba68bb2e969122778ae557d05e2ca9",
     "grade": false,
     "grade_id": "cell-e7d837a9864a6ae2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"part-2\"></a>\n",
    "# Part 2: PyTorch for Digit Classification [ 0 pts]\n",
    "\n",
    "While you were able to derive manual back-propagation rules for sigmoid and fully-connected layers, wouldn’t it be nice if someone did that for lots of useful primatives and made it fast and easy to use for general computation? Meet <a url='https://en.wikipedia.org/wiki/Automatic_differentiation'>automatic differentiation</a>. \n",
    "\n",
    "Since you have high-dimensional inputs (images) and low-dimensional outputs (a scalar loss), it turns out forward mode AD is very efficient. Popular AD packages include <a url='https://pytorch.org/'>PyTorch</a> (Facebook) and <a url=\"https://www.tensorflow.org/\"> Tensorflow</a> (Google). Autograd provides its own replacement for NumPy operators and is a drop-in replacement for NumPy, except you can ask for gradients now. The other two are able to act as shim layers for <a url=\"https://developer.nvidia.com/cudnn\">cuDNN</a>, an implementation of AD made by Nvidia for use on their GPUs. \n",
    "\n",
    "Since GPUs are able to perform large amounts of math much faster than CPUs, this makes the former two packages very popular for researchers who train large networks. Tensorflow asks you to build a computational graph using its API and then is able to pass data through that graph. PyTorch builds a dynamic graph and allows you to mix autograd functions with normal Python code much more smoothly, so it is currently more popular among people who want to build their own neural network for computer vision tasks.\n",
    "\n",
    "You will now use PyTorch as a framework. Many computer vision projects use neural networks as a basic building block, so familiarity with one of these frameworks is a good skill to develop. Here, you basically replicate and slightly expand your handwritten character recognition networks, but do it in PyTorch instead of from scratch. For this section, you will rewrite and retrain your fully-connected network on NIST36 in PyTorch and then plot training accuracy and loss over time. \n",
    "\n",
    "Although you are given the PyTorch code directly, you are encouraged to check PyTorch's <a url='https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html'>official tutorial</a> and try to train a convolutional neural network with PyTorch on different datasets such as [MNIST](http://yann.lecun.com/exdb/mnist/) or [EMNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c332c23b97f9caabf36effb2d8d4977a",
     "grade": false,
     "grade_id": "cell-5de9c43820337095",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q9\"></a>\n",
    "## PyTorch Installation [ 0 pts]\n",
    "Follow the [official guideline](https://pytorch.org/get-started/locally/) to install PyTorch and then import the necessary libraries on the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "05ed7f3871910700fa3d2315fb4a5ebb",
     "grade": false,
     "grade_id": "cell-73943af2f8e9c2d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q10\"></a>\n",
    "## Train Models with NIST36 (PyTorch) [ 0 pts]\n",
    "We already provide you the PyTorch version of training NIST36. Run the code and plot the accuracy and loss of the training and validation set. Make sure you understand the training loop and are able to relate each functional portion of the NumPy version to the PyTorch version. <b>This chunk of code will be extremely helpful if you want to utilize a neural network for classification tasks in your capstone project!<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy\n",
    "\n",
    "class MyLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(MyLayerNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(D_in, H)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_sigmoid = self.sigmoid(self.fc1(x))\n",
    "        y_pred = self.fc2(h_sigmoid)\n",
    "        return y_pred\n",
    "\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "\n",
    "max_iters = 50\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 64\n",
    "\n",
    "# PyTorch conversion\n",
    "train_ds = TensorDataset(torch.from_numpy(train_x).type(torch.float32), torch.from_numpy(train_y).type(torch.LongTensor))\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_ds = TensorDataset(torch.from_numpy(valid_x).type(torch.float32), torch.from_numpy(valid_y).type(torch.LongTensor))\n",
    "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = MyLayerNet(input_size, hidden_size, output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train_acc_list, valid_acc_list, train_loss_list, valid_loss_list = [], [], [], []\n",
    "\n",
    "for itr in range(max_iters):\n",
    "    total_loss = 0.\n",
    "    total_acc = 0.\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        ### MAKE SURE YOU KNOW THE PYTORCH IMPLEMENTATION ###\n",
    "        # forward\n",
    "        probs = model(xb)\n",
    "        gt = torch.argmax(yb, dim=1)\n",
    "        prediction = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        # loss\n",
    "        loss = loss_func(probs, gt)\n",
    "        total_loss += loss.sum().item()\n",
    "        total_acc += prediction.eq(gt.data).cpu().sum().item()\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # apply gradient\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        ####################################################\n",
    "        \n",
    "    total_loss /= len(train_loader)\n",
    "    total_acc /= train_x.shape[0]\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in valid_loader:\n",
    "            ### MAKE SURE YOU KNOW THE PYTORCH IMPLEMENTATION ###\n",
    "            probs = model(xb)\n",
    "            gt = torch.argmax(yb, dim=1)\n",
    "            prediction = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            # loss\n",
    "            loss = loss_func(probs, gt)\n",
    "            valid_loss += loss.sum().item()\n",
    "            valid_acc += prediction.eq(gt.data).cpu().sum().item()\n",
    "            ###################################################\n",
    "    valid_loss /= len(valid_loader)\n",
    "    valid_acc /= valid_y.shape[0]\n",
    "\n",
    "    train_loss_list.append(total_loss)\n",
    "    train_acc_list.append(total_acc)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    valid_acc_list.append(valid_acc)\n",
    "    if itr % 2 == 0 or itr == max_iters - 1:\n",
    "        print(\"training:: itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(itr, total_loss,total_acc))\n",
    "        print(\"validation:: itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(itr,valid_loss,valid_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1704117a56653c030a09e315717ea49d",
     "grade": false,
     "grade_id": "cell-45278e1a5873a9bf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Again, take a look at some visualizations of your accuracy over iterations. Think about what conclusions you can draw from these results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(max_iters)\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.plot(x, train_acc_list, linewidth = 2, label = \"Training Accuracy\")\n",
    "plt.plot(x, valid_acc_list, linewidth = 2, label = \"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, train_loss_list, linewidth = 2, label = \"Training Loss\")\n",
    "plt.plot(x, valid_loss_list, linewidth = 2, label = \"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a344bd8bfae0b631136013045ffaa62f",
     "grade": false,
     "grade_id": "cell-f2651e87f8daeacc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Now that you've completed the assignment, think about each essential component for building the neural network in terms of both the math formula and its functionality. Also, make sure you are able to use PyTorch to implement a simple neural network for digit classification as this modern deep learning framework will be helpful whenever you want to implement your own computer vision tasks. Take some time to try the challenge questions, where you will be asked to not only use a fully-connected network,  but a convolutional neural network to do classification on different datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c25ce408b1d0fcb8abc9ab0f9ad3fa07",
     "grade": false,
     "grade_id": "cell-7e3c45786f341004",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Challenge Question\n",
    "\n",
    "<a name=\"q10\"></a>\n",
    "## Play around with PyTorch!\n",
    "1. Train a convolutional neural network with PyTorch on the included NIST36 dataset.\n",
    "2. Train a convolutional neural network with PyTorch on MNIST. Plot training accuracy and loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now it's your turn :) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pa_template.ipynb",
   "provenance": [
    {
     "file_id": "14FWzujB9iTap8IyYndy2boJWYJFBixOT",
     "timestamp": 1627592344245
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
